{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efb69c1e-198f-43ca-a271-27653f8d4a60",
   "metadata": {},
   "source": [
    "# Stream dos dados da camada Raw para Trusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d966bf9-f69f-43a9-8c12-7601e498f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import explode, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ba1c915-bd3f-49e6-a1f4-3037c8b46c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa a sessão Spark com suporte para streaming\n",
    "spark = SparkSession.builder.appName(\"RawToTrustedStreaming\").getOrCreate()\n",
    "\n",
    "# Configura o acesso ao MinIO\n",
    "hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", \"datalake\")\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", \"datalake\")\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5dccfb2b-a937-49c4-b3c0-b8e14e21877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o caminho de entrada e saída com base na data\n",
    "fuso_horario_brasilia = pytz.timezone('America/Sao_Paulo')\n",
    "brasilia_time = datetime.now(fuso_horario_brasilia)\n",
    "today = brasilia_time.strftime('%Y-%m-%d')\n",
    "\n",
    "raw_path = f\"s3a://raw/busdata/{today}\"\n",
    "routes_trusted_path = f\"s3a://trusted/busroutes/{today}/\"\n",
    "positions_trusted_path = f\"s3a://trusted/buspositions/{today}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5642c598-8eca-4ef7-b39d-979a7d635c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o schema dos dados para evitar inferência em streaming\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"l\", ArrayType(StructType([\n",
    "        StructField(\"cl\", StringType()),\n",
    "        StructField(\"sl\", StringType()),\n",
    "        StructField(\"c\", StringType()),\n",
    "        StructField(\"lt0\", StringType()),\n",
    "        StructField(\"lt1\", StringType()),\n",
    "        StructField(\"qv\", StringType()),\n",
    "        StructField(\"vs\", ArrayType(StructType([\n",
    "            StructField(\"p\", StringType()),\n",
    "            StructField(\"ta\", StringType()),\n",
    "            StructField(\"py\", DoubleType()),\n",
    "            StructField(\"px\", DoubleType())\n",
    "        ])))\n",
    "    ])))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18de1f9e-7a40-4352-9a90-651575ea2049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura inicial do primeiro arquivo para gravar o df_bus_lines apenas uma vez\n",
    "initial_file = spark.read.schema(schema).json(raw_path).limit(1)\n",
    "df_lines = initial_file.select(explode(col(\"l\")).alias(\"linha\"))\n",
    "df_bus_lines = df_lines.select(\n",
    "    col(\"linha.cl\").alias(\"codigo_trajeto\"),\n",
    "    col(\"linha.sl\").alias(\"sentido\"),\n",
    "    col(\"linha.c\").alias(\"letreiro\"),\n",
    "    col(\"linha.lt0\").alias(\"terminal_primario\"),\n",
    "    col(\"linha.lt1\").alias(\"terminal_secundario\"),\n",
    "    col(\"linha.qv\").alias(\"qnt_veiculos\")\n",
    ")\n",
    "\n",
    "# Salva o df_bus_lines apenas uma vez no MinIO\n",
    "df_bus_lines.write.mode(\"overwrite\").json(routes_trusted_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d009fdd-5a5e-44c2-bac1-27af19feee7d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream em Execução\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Mantém o streaming ativo\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStream em Execução\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstreams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitAnyTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py:291\u001b[0m, in \u001b[0;36mStreamingQueryManager.awaitAnyTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsqm\u001b[38;5;241m.\u001b[39mawaitAnyTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsqm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitAnyTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Leitura dos arquivos no modo de streaming apenas para o processamento das posições dos veículos\n",
    "df_raw_stream = spark.readStream.schema(schema).json(raw_path)\n",
    "\n",
    "# Processamento das posições dos veículos em modo de streaming\n",
    "df_lines_stream = df_raw_stream.select(explode(col(\"l\")).alias(\"linha\"))\n",
    "df_vehicles = df_lines_stream.select(\n",
    "    col(\"linha.cl\").alias(\"codigo_trajeto\"),\n",
    "    col(\"linha.sl\").alias(\"sentido\"),\n",
    "    explode(col(\"linha.vs\")).alias(\"vehicle\")\n",
    ")\n",
    "\n",
    "df_vehicles_position = df_vehicles.select(\n",
    "    col(\"codigo_trajeto\"),\n",
    "    col(\"sentido\"),\n",
    "    col(\"vehicle.p\").alias(\"prefixo_veiculo\"),\n",
    "    col(\"vehicle.py\").alias(\"latitude\"),\n",
    "    col(\"vehicle.px\").alias(\"longitude\"),\n",
    "    col(\"vehicle.ta\").alias(\"horario_posicao\"),\n",
    ")\n",
    "\n",
    "# Grava as posições dos veículos no caminho de saída no modo de streaming\n",
    "df_vehicles_position.writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", positions_trusted_path) \\\n",
    "    .option(\"checkpointLocation\", positions_trusted_path + \"/_checkpoint\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "# Mantém o streaming ativo\n",
    "print('Stream em Execução')\n",
    "spark.streams.awaitAnyTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b42459-2706-4a8a-98a6-3115b61cd986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9ddad81-250a-4401-8682-40d58c10e22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c07f1f-18f7-4ff8-9d0a-f99ad082a45c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993961ab-dcc5-4be3-a32f-5ce71a52ece1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f148432-e7be-4bc6-a4c5-a1316cfda30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c06a6ac-cdd5-4885-8417-2bc6fe0e7fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
