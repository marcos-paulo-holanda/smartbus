{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efb69c1e-198f-43ca-a271-27653f8d4a60",
   "metadata": {},
   "source": [
    "# Stream dos dados da camada Raw para Trusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d966bf9-f69f-43a9-8c12-7601e498f650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import explode, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba1c915-bd3f-49e6-a1f4-3037c8b46c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa a sessão Spark com suporte para streaming\n",
    "spark = SparkSession.builder.appName(\"RawToTrustedStreaming\").getOrCreate()\n",
    "\n",
    "# Configura o acesso ao MinIO\n",
    "hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", \"datalake\")\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", \"datalake\")\n",
    "hadoop_conf.set(\"fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dccfb2b-a937-49c4-b3c0-b8e14e21877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o caminho de entrada e saída com base na data\n",
    "fuso_horario_brasilia = pytz.timezone('America/Sao_Paulo')\n",
    "brasilia_time = datetime.now(fuso_horario_brasilia)\n",
    "today = brasilia_time.strftime('%Y-%m-%d')\n",
    "\n",
    "raw_path = f\"s3a://raw/busdata/{today}\"\n",
    "routes_trusted_path = f\"s3a://trusted/busroutes/{today}/\"\n",
    "positions_trusted_path = f\"s3a://trusted/buspositions/{today}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5642c598-8eca-4ef7-b39d-979a7d635c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o schema dos dados para evitar inferência em streaming\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"l\", ArrayType(StructType([\n",
    "        StructField(\"cl\", StringType()),\n",
    "        StructField(\"sl\", StringType()),\n",
    "        StructField(\"c\", StringType()),\n",
    "        StructField(\"lt0\", StringType()),\n",
    "        StructField(\"lt1\", StringType()),\n",
    "        StructField(\"qv\", StringType()),\n",
    "        StructField(\"vs\", ArrayType(StructType([\n",
    "            StructField(\"p\", StringType()),\n",
    "            StructField(\"ta\", StringType()),\n",
    "            StructField(\"py\", DoubleType()),\n",
    "            StructField(\"px\", DoubleType())\n",
    "        ])))\n",
    "    ])))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18de1f9e-7a40-4352-9a90-651575ea2049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura inicial do primeiro arquivo para gravar o df_bus_lines apenas uma vez\n",
    "initial_file = spark.read.schema(schema).json(raw_path).limit(1)\n",
    "df_lines = initial_file.select(explode(col(\"l\")).alias(\"linha\"))\n",
    "df_bus_lines = df_lines.select(\n",
    "    col(\"linha.cl\").alias(\"codigo_trajeto\"),\n",
    "    col(\"linha.sl\").alias(\"sentido\"),\n",
    "    col(\"linha.c\").alias(\"letreiro\"),\n",
    "    col(\"linha.lt0\").alias(\"terminal_primario\"),\n",
    "    col(\"linha.lt1\").alias(\"terminal_secundario\"),\n",
    "    col(\"linha.qv\").alias(\"qnt_veiculos\")\n",
    ")\n",
    "\n",
    "# Salva o df_bus_lines apenas uma vez no MinIO\n",
    "df_bus_lines.write.mode(\"overwrite\").json(routes_trusted_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d009fdd-5a5e-44c2-bac1-27af19feee7d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream em Execução\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "Query [id = 1cc83bd6-a993-4909-a672-2d6184a75fe7, runId = ec10596b-7935-46a5-8b29-62fa4f1fdb62] terminated with exception: No such file or directory: s3a://raw/busdata/2024-11-04/Busdata_1328.json",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Mantém o streaming ativo\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStream em Execução\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstreams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitAnyTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py:291\u001b[0m, in \u001b[0;36mStreamingQueryManager.awaitAnyTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsqm\u001b[38;5;241m.\u001b[39mawaitAnyTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsqm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitAnyTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: Query [id = 1cc83bd6-a993-4909-a672-2d6184a75fe7, runId = ec10596b-7935-46a5-8b29-62fa4f1fdb62] terminated with exception: No such file or directory: s3a://raw/busdata/2024-11-04/Busdata_1328.json"
     ]
    }
   ],
   "source": [
    "# Leitura dos arquivos no modo de streaming apenas para o processamento das posições dos veículos\n",
    "df_raw_stream = spark.readStream.schema(schema).json(raw_path)\n",
    "\n",
    "# Processamento das posições dos veículos em modo de streaming\n",
    "df_lines_stream = df_raw_stream.select(explode(col(\"l\")).alias(\"linha\"))\n",
    "df_vehicles = df_lines_stream.select(\n",
    "    col(\"linha.cl\").alias(\"codigo_trajeto\"),\n",
    "    col(\"linha.sl\").alias(\"sentido\"),\n",
    "    explode(col(\"linha.vs\")).alias(\"vehicle\")\n",
    ")\n",
    "\n",
    "df_vehicles_position = df_vehicles.select(\n",
    "    col(\"codigo_trajeto\"),\n",
    "    col(\"sentido\"),\n",
    "    col(\"vehicle.p\").alias(\"prefixo_veiculo\"),\n",
    "    col(\"vehicle.py\").alias(\"latitude\"),\n",
    "    col(\"vehicle.px\").alias(\"longitude\"),\n",
    "    col(\"vehicle.ta\").alias(\"horario_posicao\"),\n",
    ")\n",
    "\n",
    "# Grava as posições dos veículos no caminho de saída no modo de streaming\n",
    "df_vehicles_position.writeStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"path\", positions_trusted_path) \\\n",
    "    .option(\"checkpointLocation\", positions_trusted_path + \"/_checkpoint\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "# Mantém o streaming ativo\n",
    "print('Stream em Execução')\n",
    "spark.streams.awaitAnyTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b42459-2706-4a8a-98a6-3115b61cd986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9ddad81-250a-4401-8682-40d58c10e22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c07f1f-18f7-4ff8-9d0a-f99ad082a45c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993961ab-dcc5-4be3-a32f-5ce71a52ece1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f148432-e7be-4bc6-a4c5-a1316cfda30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c06a6ac-cdd5-4885-8417-2bc6fe0e7fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
